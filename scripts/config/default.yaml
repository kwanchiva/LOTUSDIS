# Model and language settings
model_name: "nectec/Pathumma-whisper-th-large-v3"
language: "th"
task: "transcribe"

# Dataset paths
train_annotation: "/path/to/train.json"
dev_annotation: "/path/to/dev.json"

# Training parameters
training:
  output_dir: "/path/to/output"
  num_train_epochs: 5
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  learning_rate: 0.00002
  fp16: false
  bf16: true
  optim: "adamw_torch"
  eval_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 5
  logging_steps: 100
  report_to: null
  push_to_hub: false

# Audio
audio:
  target_sr: 16000
